{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pre-modeling ï¼šdata preprocessing and feature exploration in python \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  part 1:basic data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a.dealing with data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# decide which categorical var you want to use in model\n",
    "for  col in data.columns:\n",
    "    if data[col].dtype =='object':\n",
    "        unique_cat = len(data[col].unique())\n",
    "        print('feature {col}' has{unique_vat} unique categories'.foramat(col = col,unique_cat  = unique_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  you can see which var has a lot fo unique categories ,most categories only hace a few observations ,\n",
    "# inthis case bucket low frequecy categories as 'other'\n",
    "\n",
    "data['native_country'] = ['US' if x =='US' else 'other' for x in data['native_country']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of feature to dummy\n",
    "def dummy_df(df,todummy_list):\n",
    "    for x in todummy_list:\n",
    "        dummies = pd,get_dummies(data[x],prefix = x,dummy_na = False)\n",
    "        df = df.drop(x,1)\n",
    "        df= pd.concat([df,dummies],axis)= 1\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. hamdong miss data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  impute missing values using imputer in sklearn.preprocessing\n",
    "from sklearn .preprocessing import imputer\n",
    "imp = imputer(missing_values = 'nan',strategy = 'median',axis = 0)\n",
    "imp.fit(data)\n",
    "data = pd.DataFrame(data = imp.transform(data),columns = data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part 2: more data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outlier_tukey(x):\n",
    "    # x is a series\n",
    "    q1 = np.percentile(x,25)\n",
    "    q3 = np.percentile(x,75)\n",
    "    iq = q3 - q1\n",
    "    floor = q1 - 105*iqr\n",
    "    ceiling = q3+1.5*iqr\n",
    "    outlier_indices = list(x.index[(x<floor)|(x>ceiling)])\n",
    "    outlier_values = list(x[outlier_indices])\n",
    "    reture outlier_indices,outlier_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "from statsmodels.nonparamtric.kde import KDEUnivariate\n",
    "\n",
    "def find_outliers_kde(x):\n",
    "    x_scaled = scale(list(map(float ,x )))\n",
    "    kde = KDEUnivariate(x_scaled)\n",
    "    kde.fit(bw = 'scott',fft = True)\n",
    "    pred = kde.evaluate(x_scaled)\n",
    "\n",
    "    n = sum(pred < 0.05)\n",
    "    outlier_ind = np.asarray(pres).argsort()[:n]\n",
    "    outlier_value = np.asarray(x)[outlier_ind]\n",
    "    return outlier_ind,outlier_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  b. edstribution of feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  plot hist "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part 3: feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  a. interactions amongst features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  use polyomialFeaure in sklearn.preprocessing to crate two-way interactions for all features\n",
    "from itertools import combinations\n",
    "from sklearn.preprooessing import ploynomialFeatures\n",
    "\n",
    "def add_interactions(df):\n",
    "    # get feature namescombos = list(combinations(list()df.columns), 2 )\n",
    "    colnams = list(df.columns) + ['_'.join(x) for x in combos]\n",
    "\n",
    "    ploy = PolynomiaFeatures(interation_only = True, include_bias = False)\n",
    "    df = poly.fit_transfrom(df)\n",
    "    df = pd.DataFrame(df)\n",
    "    df.columns = colnames\n",
    "    # remove interraction terms with all 0 values\n",
    "    noint_indicies = [i for i, x in enmerate(list((df ==0).all())) if x]\n",
    "    df = df.drop(df.columns[noint_indicies], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. dimensinality reduction using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import __package__\n",
    "pca = PCA(n_compenents = 10)\n",
    "data_pca = pd.DataFrame(pca.fit_transform(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part 4 :feature selection and modeling building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build model using processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,train_suze = 0.7,random_seed = 200)\n",
    "\n",
    "import sklearn.feature_selection\n",
    "select = sklearn.feature_selection.SelectBest(k = 20)\n",
    "selected_features = select.fit(x_train,y_train)\n",
    "indices_selected = selected_features.get_support(indices = True)\n",
    "colnames_selected = [data.columns[i] for i in indices_selected]\n",
    "\n",
    "x_train_select = x_train[colnames_selected]\n",
    "x_test_selected = x_test[colnames_selected]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# builde your model"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4047b3199a5e836db893d769569ceb0ad5e6a904fd345e8af9e06aedd47d0a98"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
